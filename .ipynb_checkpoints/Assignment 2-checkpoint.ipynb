{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9392f52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Describe the structure of an artificial neuron. How is it similar to a biological neuron? What\n",
    "are its main components?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d36e50",
   "metadata": {},
   "source": [
    "Although artificial neurons and perceptrons were inspired by the biological processes scientists were able to observe in the brain back in the 50s, they do differ from their biological counterparts in several ways. Birds have inspired flight and horses have inspired locomotives and cars, yet none of today’s transportation vehicles resemble metal skeletons of living-breathing-self replicating animals. Still, our limited machines are even more powerful in their own domains (thus, more useful to us humans), than their animal “ancestors” could ever be. It is easy to draw the wrong conclusions from the possibilities in AI research by anthropomorphizing Deep Neural Networks, but artificial and biological neurons do differ in more ways than just the materials of their containers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8867688f",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. What are the different types of activation functions popularly used? Explain each of them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a5a045",
   "metadata": {},
   "source": [
    "Binary Step\n",
    "Linear\n",
    "Sigmoid\n",
    "Tanh\n",
    "ReLU\n",
    "Leaky ReLU\n",
    "Parameterised ReLU\n",
    "Exponential Linear Unit\n",
    "Swish\n",
    "Softmax\n",
    "\n",
    "1. Binary Step Function\n",
    "The first thing that comes to our mind when we have an activation function would be a threshold based classifier i.e. whether or not the neuron should be activated based on the value from the linear transformation.\n",
    "\n",
    "2. Linear Function\n",
    "We saw the problem with the step function, the gradient of the function became zero. This is because there is no component of x in the binary step function. Instead of a binary function, we can use a linear function. We can define the function as-\n",
    "\n",
    "3. Sigmoid\n",
    "The next activation function that we are going to look at is the Sigmoid function. It is one of the most widely used non-linear activation function. Sigmoid transforms the values between the range 0 and 1. Here is the mathematical expression for sigmoid-\n",
    "\n",
    "4. Tanh\n",
    "The tanh function is very similar to the sigmoid function. The only difference is that it is symmetric around the origin. The range of values in this case is from -1 to 1. Thus the inputs to the next layers will not always be of the same sign. The tanh function is defined as-\n",
    "\n",
    "5. ReLU\n",
    "The ReLU function is another non-linear activation function that has gained popularity in the deep learning domain. ReLU stands for Rectified Linear Unit. The main advantage of using the ReLU function over other activation functions is that it does not activate all the neurons at the same time.\n",
    "\n",
    "6. Leaky ReLU\n",
    "Leaky ReLU function is nothing but an improved version of the ReLU function. As we saw that for the ReLU function, the gradient is 0 for x<0, which would deactivate the neurons in that region.\n",
    "\n",
    "7. Parameterised ReLU\n",
    "This is another variant of ReLU that aims to solve the problem of gradient’s becoming zero for the left half of the axis. The parameterised ReLU, as the name suggests, introduces a new parameter as a slope of the negative part of the function. Here’s how the ReLU function is modified to incorporate the slope parameter-\n",
    "\n",
    "8. Exponential Linear Unit\n",
    "Exponential Linear Unit or ELU for short is also a variant of Rectiufied Linear Unit (ReLU) that modifies the slope of the negative part of the function. Unlike the leaky relu and parametric ReLU functions, instead of a straight line, ELU uses a log curve for defning the negatice values. It is defined as\n",
    "\n",
    "9.Swish\n",
    "Swish is a lesser known activation function which was discovered by researchers at Google. Swish is as computationally efficient as ReLU and shows better performance than ReLU on deeper models.  The values for swish ranges from negative infinity to infinity. The function is defined as –\n",
    "\n",
    "10. Softmax\n",
    "Softmax function is often described as a combination of multiple sigmoids. We know that sigmoid returns values between 0 and 1, which can be treated as probabilities of a data point belonging to a particular class. Thus sigmoid is widely used for binary classification problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6301bbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Explain, in details, Rosenblatt’s perceptron model. How can a set of data be classified using a\n",
    "simple perceptron?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce795d05",
   "metadata": {},
   "source": [
    "Rosenblatt perceptron is a binary single neuron model. The inputs integration is implemented through the addition of the weighted inputs that have fixed weights obtained during the training stage. If the result of this addition is larger than a given threshold θ the neuron fires. When the neuron fires its output is set to 1, otherwise it’s set to 0.\n",
    "\n",
    "The equation can be re-written as follows including what it’s known as the bias term: .\n",
    "\n",
    "Captura de pantalla 2016-08-02 a las 16.25.35\n",
    "This model implements the functioning of a single neuron that can solve linear classification problems through very simple learning algorithms. Rosenblatt Perceptrons are considered as the first generation of neural networks (the network is only compound of one neuron ☺ ). This simple single neuron model has the main limitation of not being able to solve non-linear separable problems. In my next post I will describe how this advantage was overcome and what happens when we have a layer of various perceptrons or try different neuron activation functions. Stay tuned!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1dac1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. Use a simple perceptron with weights w 0 , w 1 , and w 2  as −1, 2, and 1, respectively, to classify\n",
    "data points (3, 4); (5, 2); (1, −3); (−8, −3); (−3, 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b7586e",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. Explain the basic structure of a multi-layer perceptron. Explain how it can solve the XOR\n",
    "problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d2a670",
   "metadata": {},
   "source": [
    "A multilayer perceptron (MLP) is a class of feedforward artificial neural network (ANN). ... An MLP consists of at least three layers of nodes: an input layer, a hidden layer and an output layer. Except for the input nodes, each node is a neuron that uses a nonlinear activation function.The XOR, or “exclusive or”, problem is a classic problem in ANN research. It is the problem of using a neural network to predict the outputs of XOR logic gates given two binary inputs. An XOR function should return a true value if the two inputs are not equal and a false value if they are equal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efaa2922",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. What is artificial neural network (ANN)? Explain some of the salient highlights in the\n",
    "different architectural options for ANN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109f345d",
   "metadata": {},
   "source": [
    "The term \"Artificial Neural Network\" is derived from Biological neural networks that develop the structure of a human brain. Similar to the human brain that has neurons interconnected to one another, artificial neural networks also have neurons that are interconnected to one another in various layers of the networks. These neurons are known as nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccb649e",
   "metadata": {},
   "outputs": [],
   "source": [
    "4. Explain the learning process of an ANN. Explain, with example, the challenge in assigning\n",
    "synaptic weights for the interconnection between neurons? How can this challenge be\n",
    "addressed?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b422b0",
   "metadata": {},
   "source": [
    "An Artificial Neural Network in the field of Artificial intelligence where it attempts to mimic the network of neurons makes up a human brain so that computers will have an option to understand things and make decisions in a human-like manner. The artificial neural network is designed by programming computers to behave simply like interconnected brain cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0eb66d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "5. Explain, in details, the backpropagation algorithm. What are the limitations of this\n",
    "algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54e66f6",
   "metadata": {},
   "source": [
    "Back Propagation or back propagation of error is an algorithm for supervised learning of artificial neural networks using gradient descent. It is, though, prominently used to train the multi-layered feedforward neural networks, the main objective of the backpropagation algorithm is to adjust the weights of the neurons in the neural networks, on the basis of the given the error function, to ensure the actual output is closer to the expected result. This is performed in the form of a derivation by applying the chain rule to the error function partial derivative.\n",
    "limitation of thee backpropagation\n",
    "It relies on input to perform on a specific problem.\n",
    "Sensitive to complex/noisy data.\n",
    "It needs the derivatives of activation functions for the network design time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996a890d",
   "metadata": {},
   "outputs": [],
   "source": [
    "6. Describe, in details, the process of adjusting the interconnection weights in a multi-layer\n",
    "neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc486d3",
   "metadata": {},
   "source": [
    "The number of layers and the number of neurons are referred to as hyperparameters of a neural network, and these need tuning. Cross-validation techniques must be used to find ideal values for these.\n",
    "\n",
    "The weight adjustment training is done via backpropagation. Deeper neural networks are better at processing data. However, deeper layers can lead to vanishing gradient problems. Special algorithms are required to solve this issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80143a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "7. What are the steps in the backpropagation algorithm? Why a multi-layer neural network is\n",
    "required?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063fcb30",
   "metadata": {},
   "source": [
    "Let me summarize the steps for you:\n",
    "\n",
    "Calculate the error – How far is your model output from the actual output.\n",
    "Minimum Error – Check whether the error is minimized or not.\n",
    "Update the parameters – If the error is huge then, update the parameters (weights and biases). After that again check the error. Repeat the process until the error becomes minimum.\n",
    "Model is ready to make a prediction – Once the error becomes minimum, you can feed some inputs to your model and it will produce the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535bd598",
   "metadata": {},
   "outputs": [],
   "source": [
    "8. Write short notes on:\n",
    "\n",
    "1. Artificial neuron\n",
    "2. Multi-layer perceptron\n",
    "3. Deep learning\n",
    "4. Learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423c319d",
   "metadata": {},
   "source": [
    "Artificial neuron: \n",
    "An artificial neuron is a connection point in an artificial neural network. Artificial neural networks, like the human body's biological neural network, have a layered architecture and each network node (connection point) has the capability to process input and forward output to other nodes in the network.\n",
    "\n",
    "Multi-layer perceptron:\n",
    "The field of artificial neural networks is often just called neural networks or multi-layer perceptrons after perhaps the most useful type of neural network. A perceptron is a single neuron model that was a precursor to larger neural networks.\n",
    "It is a field that investigates how simple models of biological brains can be used to solve difficult computational tasks like the predictive modeling tasks we see in machine learning. The goal is not to create realistic models of the brain, but instead to develop robust algorithms and data structures that we can use to model difficult problems.\n",
    "\n",
    "Deep learning :\n",
    "Deep learning is a type of machine learning and artificial intelligence (AI) that imitates the way humans gain certain types of knowledge. ... While traditional machine learning algorithms are linear, deep learning algorithms are stacked in a hierarchy of increasing complexity and abstraction.\n",
    "\n",
    "Learning rate :\n",
    "Specifically, the learning rate is a configurable hyperparameter used in the training of neural networks that has a small positive value, often in the range between 0.0 and 1.0. The learning rate controls how quickly the model is adapted to the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468591c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. Write the difference between:-\n",
    "\n",
    "1. Activation function vs threshold function\n",
    "2. Step function vs sigmoid function\n",
    "3. Single layer vs multi-layer perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0baff895",
   "metadata": {},
   "source": [
    "1. Activation function vs threshold function:\n",
    "The activation function is a non-linear transformation that we do over the input before sending it to the next layer of neurons or finalizing it as output. Types of Activation Functions – Several different types of activation functions are used in Deep Learning.\n",
    "\n",
    "A threshold function is a Boolean function that determines whether a value equality of its inputs exceeded a certain threshold. A device that implements such logic is known as a threshold gate\n",
    "\n",
    "Step function vs sigmoid function\n",
    "Step function:\n",
    "Binary step function is a threshold-based activation function which means after a certain threshold neuron is activated and below the said threshold neuron is deactivated. In the above graph, the threshold is zero.\n",
    "sigmoid function:\n",
    "A Sigmoid function is a mathematical function which has a characteristic S-shaped curve. There are a number of common sigmoid functions, such as the logistic function, the hyperbolic tangent, and the arctangent\n",
    ". In machine learning, the term\n",
    "sigmoid function is normally used to refer specifically to the logistic function, also called the logistic sigmoid function.\n",
    "All sigmoid functions have the property that they map the entire number line into a small range such as between 0 and 1, or -1 and 1, so one use of a sigmoid function is to convert a real value into one that can be interpreted as a probability.\n",
    "\n",
    "3. Single layer vs multi-layer perceptron\n",
    "Single layer :\n",
    "A single-layer neural network represents the most simple form of neural network, in which there is only one layer of input nodes that send weighted inputs to a subsequent layer of receiving nodes, or in some cases, one receiving node. This single-layer design was part of the foundation for systems which have now become much more complex.\n",
    "\n",
    "multi-layer perceptron:\n",
    "A multi-layered perceptron (MLP) is one of the most common neural network models used in the field of deep learning. Often referred to as a “vanilla” neural network, an MLP is simpler than the complex models of today’s era. However, the techniques it introduced have paved the way for further advanced neural networks.\n",
    "The multilayer perceptron (MLP) is used for a variety of tasks, such as stock analysis, image identification, spam detection, and election voting predictions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
