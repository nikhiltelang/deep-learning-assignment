{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555cd550",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Is it OK to initialize all the weights to the same value as long as that value is selected\n",
    "randomly using He initialization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adec8e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. Is it OK to initialize the bias terms to 0?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0e14be",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. Name three advantages of the SELU activation function over ReLU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abf117a",
   "metadata": {},
   "source": [
    "SELU is scaled variant of ELU activation function. It uses two fixed parameters α and λ, and the value of these is derived from the inputs. However, for standardized inputs (mean of 0 and standard deviation of 1) the suggested values are α=1.6733, λ=1.0507.\n",
    "The major advantage of using SELU is that it provides self-normalization (that is output from SELU activation will preserve the mean of 0 and standard deviation of 1) and this solves the vanishing or exploding gradients problem. SELU will provide the self-normalization if: (a) the neural network consists only a stack of dense layers, (b) all the hidden layers use SELU activation function, (c) the input features must be standardized (having mean of 0 and standard deviation of 1), (d) the hidden layers weight must be initialized with LeCun normal initialization, and lastly, (e) the network must be sequential.\n",
    "However, one can start with SELU > ELU > Leaky ReLU > ReLU > tanh > sigmoid. Moreover, if one cares about the run-time then you may use Leaky ReLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f21f9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "4. In which cases would you want to use each of the following activation functions: SELU, leaky\n",
    "ReLU (and its variants), ReLU, tanh, logistic, and softmax?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5c10e5",
   "metadata": {},
   "source": [
    "your neural network architecture prevents you to meet the SELU’s self-normalizing conditions then using ELU might give better results.Leaky ReLU can also be defined as max(αx, x). The hyper-parameter alpha (α) defines how much the function leaks. Alpha is the slope of the function for x < 0 and is typically set to 0.01. This small slope ensures that leaky ReLUs never die"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4336be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "5. What may happen if you set the momentum hyperparameter too close to 1 (e.g., 0.99999)\n",
    "when using an SGD optimizer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7111752b",
   "metadata": {},
   "outputs": [],
   "source": [
    "6. Name three ways you can produce a sparse model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7719a51b",
   "metadata": {},
   "source": [
    "Sparse modeling is a component in many state of the art signal processing\n",
    "and machine learning tasks.\n",
    "• image processing (denoising, inpainting, superresolution): [Yu, Mallat, Sapiro], [Mairal, Elad, Sapiro].\n",
    "• Object recognition: [Yang, Yu, Gong, Huang], [Boureau, La Roux,\n",
    "Bach, Ponce, LeCun].\n",
    "• general supervised learning: [Mairal, Bach, Ponce, Sapiro, Zisserman].\n",
    "• Building graphs for large scale semi-supervised learning: [Liu, Wang,\n",
    "Kumar, Chang]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff11159",
   "metadata": {},
   "outputs": [],
   "source": [
    "7. Does dropout slow down training? Does it slow down inference (i.e., making predictions on\n",
    "new instances)? What about MC Dropout?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118dbda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "8. Practice training a deep neural network on the CIFAR10 image dataset:\n",
    "a. Build a DNN with 20 hidden layers of 100 neurons each (that’s too many, but it’s the\n",
    "point of this exercise). Use He initialization and the ELU activation function.\n",
    "b. Using Nadam optimization and early stopping, train the network on the CIFAR10\n",
    "dataset. You can load it with keras.datasets.cifar10.load_​data(). The dataset is\n",
    "composed of 60,000 32 × 32–pixel color images (50,000 for training, 10,000 for\n",
    "testing) with 10 classes, so you’ll need a softmax output layer with 10 neurons.\n",
    "Remember to search for the right learning rate each time you change the model’s\n",
    "architecture or hyperparameters.\n",
    "c. Now try adding Batch Normalization and compare the learning curves: Is it\n",
    "converging faster than before? Does it produce a better model? How does it affect\n",
    "training speed?\n",
    "d. Try replacing Batch Normalization with SELU, and make the necessary adjustements\n",
    "to ensure the network self-normalizes (i.e., standardize the input features, use\n",
    "LeCun normal initialization, make sure the DNN contains only a sequence of dense\n",
    "layers, etc.).\n",
    "e. Try regularizing the model with alpha dropout. Then, without retraining your model,\n",
    "see if you can achieve better accuracy using MC Dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332cf715",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972af1c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d854877",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1bf8fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
